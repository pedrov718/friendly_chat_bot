{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrov718/chatbot_for_mental_health/blob/main/generative_chatbot_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOkzWPQZ2o95"
      },
      "source": [
        "## Loading data and preliminary analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOYdKHMxxo-G"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWkNB_E2xo7N"
      },
      "source": [
        "data=pd.read_csv(\"mentalhealth.csv\",nrows=20)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Lv-3MPz-HGUA",
        "outputId": "0a385bee-73c3-4a93-89c0-d34eaafbd945"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Question_ID                                          Questions  \\\n",
              "0      1590140        What does it mean to have a mental illness?   \n",
              "1      2110618                    Who does mental illness affect?   \n",
              "2      9434130  What are some of the warning signs of mental i...   \n",
              "3      7657263            Can people with mental illness recover?   \n",
              "4      1619387  What should I do if I know someone who appears...   \n",
              "\n",
              "                                             Answers  \n",
              "0  Mental illnesses are health conditions that di...  \n",
              "1  Mental illness does can affect anyone, regardl...  \n",
              "2  Symptoms of mental health disorders vary depen...  \n",
              "3  When healing from mental illness, early identi...  \n",
              "4  We encourage those with symptoms to talk to th...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6607b23c-9b42-468a-812b-b88623133281\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question_ID</th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1590140</td>\n",
              "      <td>What does it mean to have a mental illness?</td>\n",
              "      <td>Mental illnesses are health conditions that di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2110618</td>\n",
              "      <td>Who does mental illness affect?</td>\n",
              "      <td>Mental illness does can affect anyone, regardl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9434130</td>\n",
              "      <td>What are some of the warning signs of mental i...</td>\n",
              "      <td>Symptoms of mental health disorders vary depen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7657263</td>\n",
              "      <td>Can people with mental illness recover?</td>\n",
              "      <td>When healing from mental illness, early identi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1619387</td>\n",
              "      <td>What should I do if I know someone who appears...</td>\n",
              "      <td>We encourage those with symptoms to talk to th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6607b23c-9b42-468a-812b-b88623133281')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6607b23c-9b42-468a-812b-b88623133281 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6607b23c-9b42-468a-812b-b88623133281');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/topical_chat.xlsx\")"
      ],
      "metadata": {
        "id": "LfcuRjs4TzWg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HLZBsWpVUC9z",
        "outputId": "f6379f2d-b2f4-4535-9e2e-495479cc7f6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Question\n",
              "0              Are you a fan of Google or Microsoft?\n",
              "1  Both are excellent technology they are helpful...\n",
              "2   I'm not  a huge fan of Google, but I use it a...\n",
              "3   Google provides online related services and p...\n",
              "4   Yeah, their services are good. I'm just not a..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-829b5428-3a34-43be-becf-e0f6f09693a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Are you a fan of Google or Microsoft?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Both are excellent technology they are helpful...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Google provides online related services and p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yeah, their services are good. I'm just not a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-829b5428-3a34-43be-becf-e0f6f09693a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-829b5428-3a34-43be-becf-e0f6f09693a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-829b5428-3a34-43be-becf-e0f6f09693a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace = True)"
      ],
      "metadata": {
        "id": "4Nyldz2XV1p9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = df[df['index'] % 2 == 0]\n",
        "\n",
        "answers = df[df['index'] % 2 == 1]"
      ],
      "metadata": {
        "id": "SdBYBEfWV7UB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions.reset_index(inplace = True)"
      ],
      "metadata": {
        "id": "oxYv7AcjeeGj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions.drop(columns = 'index', inplace = True)"
      ],
      "metadata": {
        "id": "RmnEIoHzXT4G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions.rename(columns = {\"Question\":\"Questions\"}, inplace = True)"
      ],
      "metadata": {
        "id": "9UNOS89WYEZb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers.drop(columns = ['index'], inplace = True)"
      ],
      "metadata": {
        "id": "-8PI2LbEXeZu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers.rename(columns = {\"Question\":\"Answers\"}, inplace = True)"
      ],
      "metadata": {
        "id": "lpNvB6cGXpOi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers.reset_index(inplace =True)"
      ],
      "metadata": {
        "id": "g60zKNZOf9t1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers.drop(columns = ['index'], inplace = True)"
      ],
      "metadata": {
        "id": "Mbwk0S3UeRTU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo = questions.join(answers)"
      ],
      "metadata": {
        "id": "sYF8H97_eXDW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge = pd.concat([convo, data], ignore_index= True)"
      ],
      "metadata": {
        "id": "C3okApXAe2mc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge.drop(columns = ['level_0'], inplace = True)"
      ],
      "metadata": {
        "id": "jJXi3ZJ1gKPP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge[\"Questions\"] = dialouge[\"Questions\"].astype(str)"
      ],
      "metadata": {
        "id": "gfIsi07Sgkvf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge[\"Answers\"] = dialouge[\"Answers\"].astype(str)"
      ],
      "metadata": {
        "id": "wubxdP64oN_l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrA850QxovH"
      },
      "source": [
        "# data preprocessing \n",
        "\n",
        "# for i in range(dialouge.shape[0]):\n",
        "#   dialouge['Answers'][i]=re.sub(r'\\n', ' ',dialouge['Answers'][i])\n",
        "#   dialouge['Answers'][i]=re.sub('\\(', '',dialouge['Answers'][i]) \n",
        "#   dialouge['Answers'][i]=re.sub(r'\\)', '',dialouge['Answers'][i]) \n",
        "#   dialouge['Answers'][i]=re.sub(r',', '',dialouge['Answers'][i]) \n",
        "#   dialouge['Answers'][i]=re.sub(r'-', '',dialouge['Answers'][i])\n",
        "#   dialouge['Answers'][i]=re.sub(r'/', '',dialouge['Answers'][i])  \n",
        "#   dialouge['Answers'][i]=re.sub(r'/', '',dialouge['Answers'][i])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge['Questions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw0LzEEQoiI4",
        "outputId": "7e137c05-1f88-46b6-9a4d-3fe534fa1956"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    Are you a fan of Google or Microsoft?\n",
              "1         I'm not  a huge fan of Google, but I use it a...\n",
              "2         Yeah, their services are good. I'm just not a...\n",
              "3        Did you know Google had hundreds of live goats...\n",
              "4        I like Google Chrome. Do you use it as well fo...\n",
              "                               ...                        \n",
              "94204                       Where can I go to find therapy\n",
              "94205    Where can I learn about types of mental health...\n",
              "94206    What are the different types of mental health ...\n",
              "94207              Where can I go to find a support group?\n",
              "94208               Where can I go to find inpatient care?\n",
              "Name: Questions, Length: 94209, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialouge['Answers']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOty0FEotbK",
        "outputId": "f9bdac55-971f-4e14-fb8b-2f4cc8aa1734"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Both are excellent technology they are helpful...\n",
              "1         Google provides online related services and p...\n",
              "2        Google is leading the alphabet subsidiary and ...\n",
              "3         It is very interesting. Google provide \"Chrom...\n",
              "4         Yes.Google is the biggest search engine and G...\n",
              "                               ...                        \n",
              "94204    Several different types of treatment and thera...\n",
              "94205    Mental health conditions are often treated wit...\n",
              "94206    There are many types of mental health professi...\n",
              "94207    Many people find peer support a helpful tool t...\n",
              "94208    Inpatient care can help people stabilize on ne...\n",
              "Name: Answers, Length: 94209, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0hDQnIg5NmU"
      },
      "source": [
        "pairs=[]\n",
        "\n",
        "for i in range(dialouge.shape[0]):\n",
        "  pairs.append(((dialouge['Questions'][i]),dialouge['Answers'][i]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6HSAiT5sGJ"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(pairs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = pairs[:1000]"
      ],
      "metadata": {
        "id": "3eXnymzCvEpn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIMImgr25tw"
      },
      "source": [
        "## Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPdw-Uwt60QW"
      },
      "source": [
        "input_docs = []\n",
        "target_docs = []\n",
        "input_tokens = set()\n",
        "target_tokens = set()\n",
        "\n",
        "for line in pairs:\n",
        "\n",
        "  input_doc, target_doc = line[0], line[1]\n",
        "\n",
        "  # Appending each input sentence to input_docs\n",
        "  input_docs.append(input_doc)\n",
        "\n",
        "  # Splitting words from punctuation  \n",
        "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
        "\n",
        "  # Redefine target_doc below and append it to target_docs\n",
        "  target_doc = '<START> ' + target_doc + ' <END>'\n",
        "\n",
        "  target_docs.append(target_doc)\n",
        "\n",
        "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
        "    if token not in input_tokens:\n",
        "      input_tokens.add(token)\n",
        "  for token in target_doc.split():\n",
        "    if token not in target_tokens:\n",
        "      target_tokens.add(token)\n",
        "\n",
        "input_tokens = sorted(list(input_tokens))  # contains all words of input_docs\n",
        "target_tokens = sorted(list(target_tokens))\n",
        "num_encoder_tokens = len(input_tokens)\n",
        "num_decoder_tokens = len(target_tokens)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BgbOOFBDXRQ",
        "outputId": "796fce1d-2207-4049-b199-bc36aabea3b9"
      },
      "source": [
        "input_docs[:20]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Did you know Jon Hamm taught drama at his high school?  Thats awesome!',\n",
              " 'i woudl probably have made the same mistake. who knew what facebook would become.',\n",
              " 'I read that! I know that certain buildings in NY have their own area codes too!',\n",
              " 'Same here!  Have a great night!',\n",
              " \" Did he really? That's kind of funny! I also think it's funny that the US president's guest house is actually bigger than the white house itself. It's like, I didn't even know the president had a guest house!\",\n",
              " \" That's true. We're not as good as hungry was back in the 50s, they had a 6-year. Where they only had one defeat. That's insane!\",\n",
              " 'I bet their desire for a look changed - he looked a bit younger!',\n",
              " 'Yes and they lay on the bottom of the ocean floor',\n",
              " \" At least he's honest.  Speaking of honesty, I think my wife only like these shows because of the actors.  Mad Men and Jon Hamm,  House and Hugh Laurie, Walking Dead because of Andrew somebody, I forget.\",\n",
              " \"Google paid 1.65 billion for youtube in 2006.  Holy smokes!  That's a lot of cashola.\",\n",
              " ' A calculator and more, we basically have access to the whole world knowledge if we know how to access it.  You could pass any academic test with a iPhone in your hands.',\n",
              " \"Was growing up when the MN Twins won the Series in '87 and '91. Still hope to see them win big again.\",\n",
              " 'Nice chatting with you!',\n",
              " 'I wonder if James Naismith ever was the highest paid one for Kansas. Even though he invented the game he was the only losing coach they ever had!',\n",
              " 'Do you like rap?',\n",
              " ' I play video games, do you like them?',\n",
              " ' Wow, I guess they have changed a lot.  Lipizzaner horses which do a lot of dressage are born black and turn white in about 6 years.  So I guess some ability to change is in their DNA',\n",
              " 'I remember using a rotary phone. Now an iPhone has more computing power than the entire US nuclear icbm deterrent.',\n",
              " 'More lies...right!',\n",
              " \" Yes.    There will probably not be a sequel though, since Jim Carrey doesn't make dramas any more.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gzh0qjP60LN",
        "outputId": "553346c2-5bd4-4fde-cdc9-46c4f4191dee"
      },
      "source": [
        "target_docs[:20]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> No , not really could you tell more about it ? <END>',\n",
              " '<START> It has definitely become a big deal . Some countries laws even include facebook . In Germany the \" like \" button is illegal . <END>',\n",
              " '<START> I guess people go overboard on phones because Unicef will donate a day of clean water to a child in need for every minute we are not on our phones . <END>',\n",
              " '<START> Hey do use Amazon ? I have Amazon Prime personally . <END>',\n",
              " \"<START> Yeah , I'm not sure if that is true . I've been on a tour of the white house . <END>\",\n",
              " \"<START> At lease we haven't placed below 3rd since 1991 <END>\",\n",
              " \"<START> True . And then Bill Nye won a Steve Martin lookalike contest which gave him a break in the entertainment industry . Honestly , I don't think they look alike ! Ha ! <END>\",\n",
              " '<START> Yeah . It was nice chatting with you <END>',\n",
              " '<START> Hilarious ! Well , did you know all of the roads in Japan are tolls ? <END>',\n",
              " '<START> It certainly is . Add to that fact , most of the content on You Tube is uploaded by individuals , the BBC , Hulu , and Vevo . And if you register on You Tube you have more options than unregistered users . <END>',\n",
              " '<START> Very true . It is amazing there was once a moral panic over reading . Imagine how they would feel about our phones ! <END>',\n",
              " \"<START> Can you believe that guy who found $ 3 million worth of baseball cards his grandfather's attic ? <END>\",\n",
              " '<START> How are you ? Do you watch the NBA , there are 30 teams in the league currently <END>',\n",
              " '<START> Thats ironic . He needed Labron then . 4 time mvp <END>',\n",
              " '<START> Not really , I used to listen to Eminem though , did you know he wanted to be a comic book artist ? <END>',\n",
              " '<START> Yes I have been playing since I was a kid , on the original Nintendo . <END>',\n",
              " \"<START> It's really amazing . People did not use to ride horses because they were so small . <END>\",\n",
              " '<START> Yes , and the iphone is better at text messages too . LOL . <END>',\n",
              " '<START> Yeah , or a way to sycke other teams to get an advantage while Brady plays . <END>',\n",
              " '<START> ah thats a good point , I guess it was the most expsensive comedy ever made too at 200 mil <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NetKOKUa6ucB"
      },
      "source": [
        "input_features_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\n",
        "target_features_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\n",
        "\n",
        "reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\n",
        "reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HjeF1rV5r1o",
        "outputId": "33407fe9-feed-4ab9-9940-ef5f9dbbd609"
      },
      "source": [
        "input_features_dict"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '\"': 1,\n",
              " '$': 2,\n",
              " '%': 3,\n",
              " '&': 4,\n",
              " \"'20s\": 5,\n",
              " \"'87\": 6,\n",
              " \"'91\": 7,\n",
              " '(': 8,\n",
              " ')': 9,\n",
              " '+': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '000': 16,\n",
              " '04': 17,\n",
              " '1': 18,\n",
              " '10': 19,\n",
              " '100': 20,\n",
              " '1000': 21,\n",
              " '12': 22,\n",
              " '12AM': 23,\n",
              " '12am': 24,\n",
              " '13': 25,\n",
              " '137': 26,\n",
              " '15': 27,\n",
              " '16': 28,\n",
              " '160': 29,\n",
              " '160mph': 30,\n",
              " '17': 31,\n",
              " '175': 32,\n",
              " '18': 33,\n",
              " \"1800's\": 34,\n",
              " '1805': 35,\n",
              " '1853': 36,\n",
              " '1869': 37,\n",
              " '1886': 38,\n",
              " \"1900's\": 39,\n",
              " '1900s': 40,\n",
              " '1916': 41,\n",
              " '1926': 42,\n",
              " '1930': 43,\n",
              " '1934': 44,\n",
              " '1940s': 45,\n",
              " '1947': 46,\n",
              " '1949': 47,\n",
              " '1957': 48,\n",
              " '1958': 49,\n",
              " '1960s': 50,\n",
              " '1969': 51,\n",
              " '1972': 52,\n",
              " '1973': 53,\n",
              " '1985': 54,\n",
              " '1986': 55,\n",
              " '1989': 56,\n",
              " '1993': 57,\n",
              " '1994': 58,\n",
              " '1995': 59,\n",
              " '1997': 60,\n",
              " '1998': 61,\n",
              " '1billion': 62,\n",
              " '1g': 63,\n",
              " '2': 64,\n",
              " '20': 65,\n",
              " '200': 66,\n",
              " '2000': 67,\n",
              " '2002': 68,\n",
              " '2003': 69,\n",
              " '2005': 70,\n",
              " '2006': 71,\n",
              " '2007': 72,\n",
              " '2011': 73,\n",
              " '2013': 74,\n",
              " '2016': 75,\n",
              " '2018': 76,\n",
              " '2022': 77,\n",
              " '20c': 78,\n",
              " '212': 79,\n",
              " '21st': 80,\n",
              " '220': 81,\n",
              " '222': 82,\n",
              " '24': 83,\n",
              " '2430': 84,\n",
              " '3': 85,\n",
              " '30': 86,\n",
              " '300': 87,\n",
              " '31': 88,\n",
              " '32': 89,\n",
              " '33': 90,\n",
              " '330000': 91,\n",
              " '3rd': 92,\n",
              " '4': 93,\n",
              " '40': 94,\n",
              " '400': 95,\n",
              " '46': 96,\n",
              " '4700': 97,\n",
              " '5': 98,\n",
              " '50': 99,\n",
              " '500k': 100,\n",
              " '50s': 101,\n",
              " '53': 102,\n",
              " '54': 103,\n",
              " '55': 104,\n",
              " '58': 105,\n",
              " '6': 106,\n",
              " '60': 107,\n",
              " \"60's\": 108,\n",
              " '600': 109,\n",
              " '62': 110,\n",
              " '65': 111,\n",
              " '6th': 112,\n",
              " '7': 113,\n",
              " '70': 114,\n",
              " \"70's\": 115,\n",
              " '700': 116,\n",
              " '70s': 117,\n",
              " '75': 118,\n",
              " '750': 119,\n",
              " '8': 120,\n",
              " '80': 121,\n",
              " \"80's\": 122,\n",
              " '80s': 123,\n",
              " '86': 124,\n",
              " '9': 125,\n",
              " '90': 126,\n",
              " '90s': 127,\n",
              " '91': 128,\n",
              " '99': 129,\n",
              " '9PM': 130,\n",
              " '9pm': 131,\n",
              " ':': 132,\n",
              " ';': 133,\n",
              " '?': 134,\n",
              " 'A': 135,\n",
              " 'AI': 136,\n",
              " 'AOC': 137,\n",
              " 'Aaron': 138,\n",
              " 'Absolutely': 139,\n",
              " 'Actually': 140,\n",
              " 'Adam': 141,\n",
              " 'Adventures': 142,\n",
              " 'Affleck': 143,\n",
              " 'Afflecks': 144,\n",
              " 'Afghanistan': 145,\n",
              " 'African': 146,\n",
              " 'After': 147,\n",
              " 'Agreed': 148,\n",
              " 'Ahh': 149,\n",
              " 'Air': 150,\n",
              " 'Al': 151,\n",
              " 'Aladdin': 152,\n",
              " 'Alan': 153,\n",
              " 'Alaska': 154,\n",
              " 'Alec': 155,\n",
              " 'Alexandria': 156,\n",
              " 'All': 157,\n",
              " 'Allen': 158,\n",
              " 'Almost': 159,\n",
              " 'Also': 160,\n",
              " 'Although': 161,\n",
              " 'Amazing': 162,\n",
              " 'Amazon': 163,\n",
              " 'Ambulances': 164,\n",
              " 'America': 165,\n",
              " \"America's\": 166,\n",
              " 'American': 167,\n",
              " 'Ancient': 168,\n",
              " 'And': 169,\n",
              " 'Andrea': 170,\n",
              " 'Andrew': 171,\n",
              " 'Anne': 172,\n",
              " 'Another': 173,\n",
              " 'Answer': 174,\n",
              " 'Antoine': 175,\n",
              " 'Any': 176,\n",
              " 'Anyway': 177,\n",
              " 'Anyways': 178,\n",
              " 'Apparently': 179,\n",
              " 'Apple': 180,\n",
              " 'Are': 181,\n",
              " 'Aretha': 182,\n",
              " 'Argentina': 183,\n",
              " 'Artificial': 184,\n",
              " 'As': 185,\n",
              " 'Asian': 186,\n",
              " 'Astronauts': 187,\n",
              " 'At': 188,\n",
              " 'Athens': 189,\n",
              " 'Atlanta': 190,\n",
              " 'August': 191,\n",
              " 'Australian': 192,\n",
              " 'Austrialian': 193,\n",
              " 'Away': 194,\n",
              " 'Aww': 195,\n",
              " 'Babe': 196,\n",
              " 'Back': 197,\n",
              " 'Baldwin': 198,\n",
              " 'Bale': 199,\n",
              " 'Ballet': 200,\n",
              " 'Bambi': 201,\n",
              " 'Banana': 202,\n",
              " 'Bart': 203,\n",
              " 'Baseball': 204,\n",
              " 'Because': 205,\n",
              " 'Been': 206,\n",
              " 'Bellatrix': 207,\n",
              " 'Ben': 208,\n",
              " 'Benz': 209,\n",
              " 'Bernstein': 210,\n",
              " 'Bezos': 211,\n",
              " 'Bible': 212,\n",
              " \"Bieber's\": 213,\n",
              " 'Big': 214,\n",
              " 'Biggie': 215,\n",
              " 'Bill': 216,\n",
              " 'Bird': 217,\n",
              " 'Black': 218,\n",
              " 'Blazing': 219,\n",
              " 'Blizzard': 220,\n",
              " 'Blockbuster': 221,\n",
              " 'Bocelli': 222,\n",
              " 'Boohoo': 223,\n",
              " 'Bowl': 224,\n",
              " 'Boy': 225,\n",
              " 'Brad': 226,\n",
              " 'Brady': 227,\n",
              " 'Braille': 228,\n",
              " 'Breath': 229,\n",
              " 'Brent': 230,\n",
              " 'Bronx': 231,\n",
              " 'Brother': 232,\n",
              " 'Browns': 233,\n",
              " 'Bruce': 234,\n",
              " 'Bryce': 235,\n",
              " 'Burger': 236,\n",
              " 'Burt': 237,\n",
              " 'Bush': 238,\n",
              " 'Busta': 239,\n",
              " 'But': 240,\n",
              " 'Butina': 241,\n",
              " 'By': 242,\n",
              " 'Bye': 243,\n",
              " 'C': 244,\n",
              " 'CDs': 245,\n",
              " 'CNN': 246,\n",
              " 'Cadillac': 247,\n",
              " 'Cadillacs': 248,\n",
              " 'Caine': 249,\n",
              " 'Calais': 250,\n",
              " 'Call': 251,\n",
              " 'Campbell': 252,\n",
              " 'Can': 253,\n",
              " \"Can't\": 254,\n",
              " 'Canada': 255,\n",
              " 'Canadian': 256,\n",
              " 'Canda': 257,\n",
              " 'Cannan': 258,\n",
              " 'Captain': 259,\n",
              " 'Care': 260,\n",
              " 'Carlin': 261,\n",
              " 'Carrey': 262,\n",
              " 'Carson': 263,\n",
              " 'Carter': 264,\n",
              " 'Cast': 265,\n",
              " 'Cell': 266,\n",
              " 'Ceres': 267,\n",
              " 'Champion': 268,\n",
              " 'Chan': 269,\n",
              " 'Chaos': 270,\n",
              " 'Chapman': 271,\n",
              " 'Chekov': 272,\n",
              " 'Cheryl': 273,\n",
              " \"Chicago's\": 274,\n",
              " 'Chilling': 275,\n",
              " 'China': 276,\n",
              " 'Christian': 277,\n",
              " 'Chromebook': 278,\n",
              " 'Church': 279,\n",
              " 'City': 280,\n",
              " 'Cleveland': 281,\n",
              " 'Clint': 282,\n",
              " 'Clinton': 283,\n",
              " 'Clintons': 284,\n",
              " 'Cobb': 285,\n",
              " 'Cohen': 286,\n",
              " 'Collinsville': 287,\n",
              " 'Color': 288,\n",
              " 'Comedic': 289,\n",
              " 'Communist': 290,\n",
              " 'Company': 291,\n",
              " 'Computer': 292,\n",
              " 'Considering': 293,\n",
              " 'Cornell': 294,\n",
              " 'Coruscant': 295,\n",
              " 'Could': 296,\n",
              " 'Court': 297,\n",
              " 'Crazy': 298,\n",
              " 'Croatia': 299,\n",
              " 'Cruise': 300,\n",
              " 'Cubs': 301,\n",
              " 'Cumberland': 302,\n",
              " 'Cup': 303,\n",
              " 'Currently': 304,\n",
              " 'D': 305,\n",
              " 'DC': 306,\n",
              " 'DId': 307,\n",
              " 'DNA': 308,\n",
              " 'DO': 309,\n",
              " 'DVD': 310,\n",
              " 'DVDs': 311,\n",
              " 'Dakwandre': 312,\n",
              " 'Dalmatians': 313,\n",
              " 'Dalton': 314,\n",
              " 'Danny': 315,\n",
              " 'David': 316,\n",
              " 'De': 317,\n",
              " 'Dead': 318,\n",
              " 'Definately': 319,\n",
              " 'Demetri': 320,\n",
              " 'Democrat': 321,\n",
              " 'Depending': 322,\n",
              " 'Depp': 323,\n",
              " 'Detroit': 324,\n",
              " 'Dictators': 325,\n",
              " 'Did': 326,\n",
              " \"Didn't\": 327,\n",
              " 'Didnt': 328,\n",
              " 'Disney': 329,\n",
              " 'Do': 330,\n",
              " 'Does': 331,\n",
              " \"Doesn't\": 332,\n",
              " 'Dogs': 333,\n",
              " 'Dominican': 334,\n",
              " 'Doom': 335,\n",
              " 'Doritos': 336,\n",
              " 'Dorothy': 337,\n",
              " \"Dragon's\": 338,\n",
              " 'Drifting': 339,\n",
              " 'Dumbo': 340,\n",
              " 'During': 341,\n",
              " 'Dutch': 342,\n",
              " 'Duty': 343,\n",
              " 'Dwight': 344,\n",
              " 'ESPN': 345,\n",
              " 'Each': 346,\n",
              " 'Eagles': 347,\n",
              " 'Early': 348,\n",
              " 'Earth': 349,\n",
              " 'Eastwood': 350,\n",
              " 'Eddie': 351,\n",
              " 'Edgar': 352,\n",
              " 'Edna': 353,\n",
              " 'El': 354,\n",
              " \"Eli's\": 355,\n",
              " 'Ellie': 356,\n",
              " 'Elon': 357,\n",
              " \"Ender's\": 358,\n",
              " 'Enders': 359,\n",
              " 'England': 360,\n",
              " 'English': 361,\n",
              " 'Ethics': 362,\n",
              " 'Europe': 363,\n",
              " 'Evans': 364,\n",
              " 'Even': 365,\n",
              " 'Ever': 366,\n",
              " 'Everyone': 367,\n",
              " 'Ew': 368,\n",
              " 'FDR': 369,\n",
              " 'FIFA': 370,\n",
              " 'FXX': 371,\n",
              " 'Facebook': 372,\n",
              " 'Fahey': 373,\n",
              " 'Falls': 374,\n",
              " 'Families': 375,\n",
              " 'Family': 376,\n",
              " 'Fantasy': 377,\n",
              " 'Fascinating': 378,\n",
              " 'Female': 379,\n",
              " 'Fish': 380,\n",
              " 'Flame': 381,\n",
              " 'Floyd': 382,\n",
              " 'Football': 383,\n",
              " 'For': 384,\n",
              " 'Ford': 385,\n",
              " \"Ford's\": 386,\n",
              " 'Fox': 387,\n",
              " 'Foxxhole': 388,\n",
              " 'France': 389,\n",
              " 'Francisco': 390,\n",
              " 'Franklin': 391,\n",
              " 'Free': 392,\n",
              " 'French': 393,\n",
              " 'Friends': 394,\n",
              " 'Frozen': 395,\n",
              " 'Fry': 396,\n",
              " 'GOP': 397,\n",
              " 'Gaelic': 398,\n",
              " 'Gaga': 399,\n",
              " 'Gaia': 400,\n",
              " 'Galaxy': 401,\n",
              " 'Game': 402,\n",
              " 'Gameboy': 403,\n",
              " 'Games': 404,\n",
              " 'Gangnam': 405,\n",
              " 'Gehrig': 406,\n",
              " 'George': 407,\n",
              " \"Georgetown's\": 408,\n",
              " 'Georgia': 409,\n",
              " 'German': 410,\n",
              " 'Gervais': 411,\n",
              " 'Giant': 412,\n",
              " 'Gives': 413,\n",
              " 'Gladstone': 414,\n",
              " 'Go': 415,\n",
              " 'Golden': 416,\n",
              " 'Good': 417,\n",
              " 'Google': 418,\n",
              " \"Google's\": 419,\n",
              " 'Grace': 420,\n",
              " 'Graham': 421,\n",
              " 'Great': 422,\n",
              " 'Group': 423,\n",
              " 'Guest': 424,\n",
              " 'Guide': 425,\n",
              " 'Guy': 426,\n",
              " 'HA': 427,\n",
              " 'HAHA': 428,\n",
              " 'Ha': 429,\n",
              " 'Haha': 430,\n",
              " 'Hahah': 431,\n",
              " 'Hahaha': 432,\n",
              " 'Halloween': 433,\n",
              " 'Hamlet': 434,\n",
              " 'Hamm': 435,\n",
              " 'Hammer': 436,\n",
              " 'Hard': 437,\n",
              " 'Harold': 438,\n",
              " 'Harris': 439,\n",
              " 'Harry': 440,\n",
              " 'Harvard': 441,\n",
              " 'Hate': 442,\n",
              " 'Have': 443,\n",
              " 'Havent': 444,\n",
              " 'Hawaii': 445,\n",
              " 'Haywood': 446,\n",
              " 'He': 447,\n",
              " \"He's\": 448,\n",
              " 'Heard': 449,\n",
              " 'Heart': 450,\n",
              " 'Hello': 451,\n",
              " 'Her': 452,\n",
              " 'Here': 453,\n",
              " 'Hes': 454,\n",
              " 'Hey': 455,\n",
              " 'Hi': 456,\n",
              " 'Hickory': 457,\n",
              " 'High': 458,\n",
              " 'Highest': 459,\n",
              " 'Hilary': 460,\n",
              " 'Hindenburg': 461,\n",
              " 'His': 462,\n",
              " \"Hitchhiker's\": 463,\n",
              " 'Hm': 464,\n",
              " 'Hmm': 465,\n",
              " 'Hmmm': 466,\n",
              " 'Hollywood': 467,\n",
              " 'Holocaust': 468,\n",
              " 'Holy': 469,\n",
              " 'Homer': 470,\n",
              " 'Honestly': 471,\n",
              " 'Hopper': 472,\n",
              " 'Horror': 473,\n",
              " 'Hoth': 474,\n",
              " 'House': 475,\n",
              " 'How': 476,\n",
              " \"How's\": 477,\n",
              " 'Howard': 478,\n",
              " 'However': 479,\n",
              " 'Hows': 480,\n",
              " 'Hugh': 481,\n",
              " 'Hybrid': 482,\n",
              " 'I': 483,\n",
              " \"I'd\": 484,\n",
              " \"I'll\": 485,\n",
              " \"I'm\": 486,\n",
              " \"I've\": 487,\n",
              " 'ICBM': 488,\n",
              " 'IGN': 489,\n",
              " 'IL': 490,\n",
              " 'Iceland': 491,\n",
              " 'Icelanders': 492,\n",
              " 'Icelandic': 493,\n",
              " 'Id': 494,\n",
              " 'If': 495,\n",
              " 'Il': 496,\n",
              " 'Ill': 497,\n",
              " 'Im': 498,\n",
              " 'In': 499,\n",
              " 'Inception': 500,\n",
              " 'Incredibles': 501,\n",
              " 'Individual': 502,\n",
              " 'Indonesia': 503,\n",
              " 'Infinity': 504,\n",
              " 'Instead': 505,\n",
              " 'Insurance': 506,\n",
              " 'Intelligence': 507,\n",
              " 'Interesting': 508,\n",
              " 'Internet': 509,\n",
              " 'Ireland': 510,\n",
              " 'Ironically': 511,\n",
              " 'Is': 512,\n",
              " 'Island': 513,\n",
              " 'Islandborn': 514,\n",
              " 'It': 515,\n",
              " \"It's\": 516,\n",
              " 'Its': 517,\n",
              " 'It': 518,\n",
              " 'Ive': 519,\n",
              " 'JFK': 520,\n",
              " 'Jack': 521,\n",
              " \"Jack's\": 522,\n",
              " 'Jackie': 523,\n",
              " 'Jackson': 524,\n",
              " 'Jaguars': 525,\n",
              " 'James': 526,\n",
              " \"James'\": 527,\n",
              " 'Jann': 528,\n",
              " 'Japan': 529,\n",
              " 'Japanese': 530,\n",
              " 'Jaws': 531,\n",
              " 'Jazz': 532,\n",
              " 'Jeff': 533,\n",
              " 'Jefferson': 534,\n",
              " \"Jefferson's\": 535,\n",
              " 'Jeopardy': 536,\n",
              " 'Jersey': 537,\n",
              " 'Jim': 538,\n",
              " 'Jimmy': 539,\n",
              " 'Joe': 540,\n",
              " 'John': 541,\n",
              " 'Johnny': 542,\n",
              " 'Johnson': 543,\n",
              " 'Jon': 544,\n",
              " 'Jones': 545,\n",
              " 'Jonez': 546,\n",
              " 'Jong': 547,\n",
              " 'Jordan': 548,\n",
              " \"Jordan's\": 549,\n",
              " 'Journalists': 550,\n",
              " 'Judge': 551,\n",
              " 'Judy': 552,\n",
              " 'Juliet': 553,\n",
              " 'Juno': 554,\n",
              " 'Jupiter': 555,\n",
              " 'Jurrasic': 556,\n",
              " 'Just': 557,\n",
              " 'Justin': 558,\n",
              " 'K': 559,\n",
              " 'Kalakaua': 560,\n",
              " 'Kansas': 561,\n",
              " 'Kanye': 562,\n",
              " 'Katy': 563,\n",
              " 'Kemper': 564,\n",
              " 'Kendrick': 565,\n",
              " 'Key': 566,\n",
              " 'Kickball': 567,\n",
              " 'Kid': 568,\n",
              " 'Kids': 569,\n",
              " 'Kim': 570,\n",
              " \"Kim's\": 571,\n",
              " 'Kimmel': 572,\n",
              " 'Kind': 573,\n",
              " 'King': 574,\n",
              " 'Kingdom': 575,\n",
              " 'Knighted': 576,\n",
              " 'Korea': 577,\n",
              " 'Korean': 578,\n",
              " 'Koren': 579,\n",
              " 'Kuiper': 580,\n",
              " 'Kumar': 581,\n",
              " 'Kurt': 582,\n",
              " 'LIkewise': 583,\n",
              " 'LOL': 584,\n",
              " 'LPs': 585,\n",
              " 'La': 586,\n",
              " 'Lady': 587,\n",
              " 'Lamar': 588,\n",
              " 'Land': 589,\n",
              " 'Las': 590,\n",
              " 'Latin': 591,\n",
              " 'Laurie': 592,\n",
              " 'LeBron': 593,\n",
              " 'Lebron': 594,\n",
              " 'Lee': 595,\n",
              " 'Leia': 596,\n",
              " \"Let's\": 597,\n",
              " 'Library': 598,\n",
              " 'Like': 599,\n",
              " 'Liked': 600,\n",
              " 'Lillyhammer': 601,\n",
              " 'Lilyhammer': 602,\n",
              " 'Lincoln': 603,\n",
              " 'Linkin': 604,\n",
              " 'Lipizzaner': 605,\n",
              " 'Literature': 606,\n",
              " 'Little': 607,\n",
              " 'Lol': 608,\n",
              " 'Longest': 609,\n",
              " 'Looks': 610,\n",
              " 'Lou': 611,\n",
              " 'Love': 612,\n",
              " 'Luck': 613,\n",
              " 'MC': 614,\n",
              " 'MD': 615,\n",
              " 'MJ': 616,\n",
              " 'MLB': 617,\n",
              " 'MN': 618,\n",
              " 'MSNBC': 619,\n",
              " 'Mackinac': 620,\n",
              " 'Mad': 621,\n",
              " 'Makes': 622,\n",
              " 'Marie': 623,\n",
              " 'Mars': 624,\n",
              " 'Marshall': 625,\n",
              " 'Martin': 626,\n",
              " 'Marvel': 627,\n",
              " 'Maverick': 628,\n",
              " 'Maybe': 629,\n",
              " 'McCarrick': 630,\n",
              " \"McD's\": 631,\n",
              " \"McDonald's\": 632,\n",
              " 'McDonalds': 633,\n",
              " 'McKenzie': 634,\n",
              " 'Mcdonald': 635,\n",
              " 'Me': 636,\n",
              " 'Meanwhile': 637,\n",
              " 'Megalodons': 638,\n",
              " 'Men': 639,\n",
              " 'Meryl': 640,\n",
              " 'Mia': 641,\n",
              " 'Michael': 642,\n",
              " 'Michal': 643,\n",
              " 'Michigan': 644,\n",
              " 'Microsoft': 645,\n",
              " 'Mid': 646,\n",
              " 'Miller': 647,\n",
              " 'Million': 648,\n",
              " 'Minister': 649,\n",
              " 'Miranda': 650,\n",
              " 'Mobile': 651,\n",
              " 'Mode': 652,\n",
              " 'Model': 653,\n",
              " 'Momma': 654,\n",
              " 'Monaco': 655,\n",
              " 'Moore': 656,\n",
              " 'More': 657,\n",
              " 'Morocco': 658,\n",
              " 'Moss': 659,\n",
              " 'Mostly': 660,\n",
              " 'Mothe': 661,\n",
              " 'Movie': 662,\n",
              " 'Mr': 663,\n",
              " \"Munk's\": 664,\n",
              " 'Murphy': 665,\n",
              " 'Murray': 666,\n",
              " 'Musk': 667,\n",
              " 'Must': 668,\n",
              " 'My': 669,\n",
              " 'N': 670,\n",
              " 'NBA': 671,\n",
              " 'NES': 672,\n",
              " 'NEtflix': 673,\n",
              " 'NFL': 674,\n",
              " 'NO': 675,\n",
              " 'NY': 676,\n",
              " \"NY's\": 677,\n",
              " 'Naismith': 678,\n",
              " 'Napoleon': 679,\n",
              " 'National': 680,\n",
              " 'Nationalist': 681,\n",
              " 'Netflix': 682,\n",
              " 'Nevada': 683,\n",
              " 'Neville': 684,\n",
              " 'New': 685,\n",
              " 'Nice': 686,\n",
              " 'Nickelodeon': 687,\n",
              " 'Nights': 688,\n",
              " 'Nike': 689,\n",
              " 'No': 690,\n",
              " 'North': 691,\n",
              " 'Norwegian': 692,\n",
              " 'Not': 693,\n",
              " 'Nothing': 694,\n",
              " 'Now': 695,\n",
              " 'Nutty': 696,\n",
              " \"O'\": 697,\n",
              " \"O'Brother\": 698,\n",
              " 'October': 699,\n",
              " 'Of': 700,\n",
              " 'Office': 701,\n",
              " 'Oh': 702,\n",
              " 'Ohh': 703,\n",
              " 'Ok': 704,\n",
              " 'Okay': 705,\n",
              " 'Old': 706,\n",
              " 'Olympic': 707,\n",
              " 'On': 708,\n",
              " 'Once': 709,\n",
              " 'One': 710,\n",
              " 'Only': 711,\n",
              " 'Ooooo': 712,\n",
              " 'Or': 713,\n",
              " 'Orlando': 714,\n",
              " 'Ouch': 715,\n",
              " \"PC's\": 716,\n",
              " 'POW': 717,\n",
              " 'Pallas': 718,\n",
              " 'Panther': 719,\n",
              " 'Parents': 720,\n",
              " 'Park': 721,\n",
              " 'Pashinyan': 722,\n",
              " 'Passionate': 723,\n",
              " 'Pats': 724,\n",
              " 'People': 725,\n",
              " 'Perhaps': 726,\n",
              " 'Perry': 727,\n",
              " 'Personally': 728,\n",
              " 'Pete': 729,\n",
              " 'PhD': 730,\n",
              " 'Phil': 731,\n",
              " 'Pilgrims': 732,\n",
              " 'Pink': 733,\n",
              " 'Pixar': 734,\n",
              " 'Planet': 735,\n",
              " 'Planets': 736,\n",
              " 'Play': 737,\n",
              " 'PlayStation': 738,\n",
              " 'Player': 739,\n",
              " 'Pluto': 740,\n",
              " 'Poe': 741,\n",
              " 'Poetry': 742,\n",
              " 'Pokemon': 743,\n",
              " 'Politics': 744,\n",
              " 'Pompeii': 745,\n",
              " 'Possibly': 746,\n",
              " 'Post': 747,\n",
              " 'Potter': 748,\n",
              " 'Pratchett': 749,\n",
              " 'President': 750,\n",
              " \"President's\": 751,\n",
              " 'Presidential': 752,\n",
              " 'Presumably': 753,\n",
              " 'Pretty': 754,\n",
              " 'Prime': 755,\n",
              " 'Princess': 756,\n",
              " 'Probably': 757,\n",
              " 'Problem': 758,\n",
              " 'Professional': 759,\n",
              " 'Professor': 760,\n",
              " 'QB': 761,\n",
              " 'Qaeda': 762,\n",
              " 'Qatar': 763,\n",
              " 'Quintilian': 764,\n",
              " 'Quite': 765,\n",
              " 'RR': 766,\n",
              " 'Radios': 767,\n",
              " 'Ramones': 768,\n",
              " \"Rand's\": 769,\n",
              " 'Randy': 770,\n",
              " 'Reading': 771,\n",
              " 'Reagan': 772,\n",
              " 'Really': 773,\n",
              " 'Red': 774,\n",
              " 'Reddit': 775,\n",
              " 'Reel': 776,\n",
              " 'Reggie': 777,\n",
              " 'Reno': 778,\n",
              " 'Reptar': 779,\n",
              " 'Republican': 780,\n",
              " 'Rex': 781,\n",
              " 'Reynolds': 782,\n",
              " 'Rhymes': 783,\n",
              " 'Rick': 784,\n",
              " 'Ricky': 785,\n",
              " 'Right': 786,\n",
              " 'Ringo': 787,\n",
              " 'Robert': 788,\n",
              " 'Robin': 789,\n",
              " 'Rock': 790,\n",
              " 'Rodgers': 791,\n",
              " 'Rogers': 792,\n",
              " 'Rollins': 793,\n",
              " 'Roman': 794,\n",
              " 'Romeo': 795,\n",
              " 'Ronald': 796,\n",
              " 'Roos': 797,\n",
              " 'Rugrats': 798,\n",
              " 'Russia': 799,\n",
              " \"Russia's\": 800,\n",
              " 'Russian': 801,\n",
              " 'Ruth': 802,\n",
              " 'S': 803,\n",
              " 'Sabrina': 804,\n",
              " 'Saddles': 805,\n",
              " 'Same': 806,\n",
              " 'San': 807,\n",
              " 'Sandler': 808,\n",
              " 'Save': 809,\n",
              " 'Scary': 810,\n",
              " 'Schmitt': 811,\n",
              " 'School': 812,\n",
              " 'Science': 813,\n",
              " 'Scientists': 814,\n",
              " 'Scotland': 815,\n",
              " 'Scott': 816,\n",
              " 'Scottland': 817,\n",
              " 'Seemed': 818,\n",
              " 'Seems': 819,\n",
              " 'Seinfeld': 820,\n",
              " 'Senate': 821,\n",
              " 'Series': 822,\n",
              " 'Serires': 823,\n",
              " 'Service': 824,\n",
              " 'Seymour': 825,\n",
              " 'Shadow': 826,\n",
              " 'Shakespeare': 827,\n",
              " 'Shark': 828,\n",
              " 'Shawn': 829,\n",
              " 'She': 830,\n",
              " 'Sherlock': 831,\n",
              " 'Shining': 832,\n",
              " 'Show': 833,\n",
              " \"Simpson's\": 834,\n",
              " 'Simpsons': 835,\n",
              " 'Since': 836,\n",
              " 'Skellington': 837,\n",
              " 'Sketchy': 838,\n",
              " 'Smart': 839,\n",
              " 'Smith': 840,\n",
              " \"Snoop's\": 841,\n",
              " 'Snyder': 842,\n",
              " 'So': 843,\n",
              " 'Soccer': 844,\n",
              " 'Some': 845,\n",
              " 'Somehow': 846,\n",
              " 'Someone': 847,\n",
              " 'Something': 848,\n",
              " 'Sometimes': 849,\n",
              " 'Sonny': 850,\n",
              " 'Sorry': 851,\n",
              " 'Sound': 852,\n",
              " 'Sounds': 853,\n",
              " 'South': 854,\n",
              " 'Southhaven': 855,\n",
              " 'Sox': 856,\n",
              " 'SpaceX': 857,\n",
              " 'Spain': 858,\n",
              " 'Sparrow': 859,\n",
              " 'Speaking': 860,\n",
              " 'Spencer': 861,\n",
              " 'Spider': 862,\n",
              " 'Spiderman': 863,\n",
              " 'Spielberg': 864,\n",
              " 'Spike': 865,\n",
              " 'Springfield': 866,\n",
              " \"Stanford's\": 867,\n",
              " 'Stapp': 868,\n",
              " 'Star': 869,\n",
              " 'Stark': 870,\n",
              " 'Starship': 871,\n",
              " 'State': 872,\n",
              " 'States': 873,\n",
              " 'Steep': 874,\n",
              " 'Stepen': 875,\n",
              " 'Stephen': 876,\n",
              " 'Steve': 877,\n",
              " 'Still': 878,\n",
              " 'Stough': 879,\n",
              " 'Such': 880,\n",
              " 'Super': 881,\n",
              " 'Supposedly': 882,\n",
              " 'Sure': 883,\n",
              " 'Swift': 884,\n",
              " 'Switching': 885,\n",
              " 'T': 886,\n",
              " 'THats': 887,\n",
              " 'TRUMP': 888,\n",
              " 'TV': 889,\n",
              " 'Take': 890,\n",
              " 'Takes': 891,\n",
              " 'Tale': 892,\n",
              " 'Talk': 893,\n",
              " 'Talkeetna': 894,\n",
              " 'Tank': 895,\n",
              " 'Tanya': 896,\n",
              " 'Tatooine': 897,\n",
              " 'Taylor': 898,\n",
              " 'Tech': 899,\n",
              " 'Tegan': 900,\n",
              " 'Telephone': 901,\n",
              " 'Television': 902,\n",
              " 'Tell': 903,\n",
              " 'Tellus': 904,\n",
              " 'Terry': 905,\n",
              " 'Thank': 906,\n",
              " 'Thanks': 907,\n",
              " 'That': 908,\n",
              " \"That's\": 909,\n",
              " 'Thats': 910,\n",
              " 'The': 911,\n",
              " 'Their': 912,\n",
              " 'Then': 913,\n",
              " 'Theory': 914,\n",
              " 'There': 915,\n",
              " \"There's\": 916,\n",
              " 'These': 917,\n",
              " 'They': 918,\n",
              " \"They've\": 919,\n",
              " 'This': 920,\n",
              " 'Thomas': 921,\n",
              " 'Those': 922,\n",
              " 'Though': 923,\n",
              " 'Thrones': 924,\n",
              " 'Through': 925,\n",
              " 'Tiger': 926,\n",
              " 'Timberlands': 927,\n",
              " 'To': 928,\n",
              " 'Today': 929,\n",
              " 'Tofu': 930,\n",
              " 'Together': 931,\n",
              " 'Tom': 932,\n",
              " 'Tony': 933,\n",
              " 'Too': 934,\n",
              " 'Tooth': 935,\n",
              " 'Treasure': 936,\n",
              " 'Trek': 937,\n",
              " 'Troopers': 938,\n",
              " 'Truman': 939,\n",
              " 'Trump': 940,\n",
              " 'Trumps': 941,\n",
              " 'Tube': 942,\n",
              " 'Tucker': 943,\n",
              " 'Tupac': 944,\n",
              " 'Turkey': 945,\n",
              " 'Twins': 946,\n",
              " 'U': 947,\n",
              " 'UK': 948,\n",
              " 'US': 949,\n",
              " 'Un': 950,\n",
              " 'Unfortunately': 951,\n",
              " 'Unicef': 952,\n",
              " 'United': 953,\n",
              " 'Universal': 954,\n",
              " 'Universe': 955,\n",
              " 'University': 956,\n",
              " 'Unlike': 957,\n",
              " 'Us': 958,\n",
              " 'Vaguely': 959,\n",
              " 'Valuable': 960,\n",
              " 'Vegas': 961,\n",
              " 'Venus': 962,\n",
              " 'Very': 963,\n",
              " 'Vesta': 964,\n",
              " 'Vice': 965,\n",
              " 'Victorious': 966,\n",
              " 'Village': 967,\n",
              " 'Voting': 968,\n",
              " 'W': 969,\n",
              " 'WAY': 970,\n",
              " 'WOW': 971,\n",
              " 'WOw': 972,\n",
              " 'WW2': 973,\n",
              " 'WWII': 974,\n",
              " 'Wagon': 975,\n",
              " 'Walking': 976,\n",
              " 'Walt': 977,\n",
              " 'Want': 978,\n",
              " 'War': 979,\n",
              " 'Warner': 980,\n",
              " 'Warriors': 981,\n",
              " 'Wars': 982,\n",
              " 'Was': 983,\n",
              " 'Washington': 984,\n",
              " \"Wasn't\": 985,\n",
              " 'Watchmen': 986,\n",
              " 'Wavyy': 987,\n",
              " 'We': 988,\n",
              " \"We're\": 989,\n",
              " \"We've\": 990,\n",
              " 'Well': 991,\n",
              " 'Were': 992,\n",
              " 'West': 993,\n",
              " 'What': 994,\n",
              " \"What's\": 995,\n",
              " 'Whatever': 996,\n",
              " 'When': 997,\n",
              " 'Where': 998,\n",
              " 'Which': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1xvXlXR3Rjo"
      },
      "source": [
        "## Encoder - Decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4SCSarcDJm3"
      },
      "source": [
        "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
        "#max_encoder_seq_length = 100\n",
        "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
        "#max_decoder_seq_length = 100\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
        "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
        "        #Assign 1. for the current line, timestep, & word in encoder_input_data\n",
        "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
        "    \n",
        "    for timestep, token in enumerate(target_doc.split()):\n",
        "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
        "        if timestep > 0:\n",
        "            decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_TCl5B2c9Vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21735939-aec0-471a-f294-32f2e11a1366"
      },
      "source": [
        "encoder_input_data"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLXrUXkxOehp",
        "outputId": "a84c6ca7-ffad-46b0-95f4-8606de08b465"
      },
      "source": [
        "decoder_target_data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M74GLqQ32ET"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADU7bfIXOefe"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model  \n",
        "dimensionality = 256 # Dimensionality \n",
        "batch_size = 10   # The batch size and number of epochs\n",
        "epochs = 500 \n",
        "\n",
        "#Encoder\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "#Decoder\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA1W1_-HOecW"
      },
      "source": [
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # Compiling"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9EEk4_KLLy2",
        "outputId": "396299cd-77de-4e70-a50d-4ce19b92e5eb"
      },
      "source": [
        "training_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 3680)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 3866)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        4031488     ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  4221952     ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 3866)   993562      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,247,002\n",
            "Trainable params: 9,247,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "n9HeMTwusJ-Y",
        "outputId": "590ef886-088f-4c37-e14a-c2a172662fdc"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(training_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)   # plot model"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SzwmitzgJ4zj",
        "outputId": "66e4a92e-e3f9-4be8-9dab-553ef2f69480"
      },
      "source": [
        "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')#Training\n",
        "\n",
        "history1=training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
        "\n",
        "training_model.save('training_model.h5')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "80/80 [==============================] - 13s 65ms/step - loss: 1.5765 - accuracy: 0.0160 - val_loss: 1.4902 - val_accuracy: 0.0142\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5328 - accuracy: 0.0152 - val_loss: 1.5090 - val_accuracy: 0.0139\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 4s 46ms/step - loss: 1.5244 - accuracy: 0.0150 - val_loss: 1.5867 - val_accuracy: 0.0117\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5189 - accuracy: 0.0151 - val_loss: 1.5577 - val_accuracy: 0.0143\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5129 - accuracy: 0.0155 - val_loss: 1.5778 - val_accuracy: 0.0142\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5110 - accuracy: 0.0155 - val_loss: 1.5762 - val_accuracy: 0.0139\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5071 - accuracy: 0.0155 - val_loss: 1.5813 - val_accuracy: 0.0140\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5239 - accuracy: 0.0154 - val_loss: 1.5885 - val_accuracy: 0.0141\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5057 - accuracy: 0.0154 - val_loss: 1.5832 - val_accuracy: 0.0140\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5035 - accuracy: 0.0155 - val_loss: 1.5904 - val_accuracy: 0.0141\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5013 - accuracy: 0.0158 - val_loss: 1.5851 - val_accuracy: 0.0141\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5014 - accuracy: 0.0156 - val_loss: 1.5941 - val_accuracy: 0.0144\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4966 - accuracy: 0.0154 - val_loss: 1.5851 - val_accuracy: 0.0141\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.5016 - accuracy: 0.0156 - val_loss: 1.5851 - val_accuracy: 0.0142\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4976 - accuracy: 0.0158 - val_loss: 1.5840 - val_accuracy: 0.0142\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4941 - accuracy: 0.0159 - val_loss: 1.5906 - val_accuracy: 0.0140\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.5017 - accuracy: 0.0155 - val_loss: 1.5844 - val_accuracy: 0.0144\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4936 - accuracy: 0.0158 - val_loss: 1.5878 - val_accuracy: 0.0141\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4924 - accuracy: 0.0155 - val_loss: 1.5887 - val_accuracy: 0.0141\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4907 - accuracy: 0.0159 - val_loss: 1.5865 - val_accuracy: 0.0146\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4865 - accuracy: 0.0159 - val_loss: 1.5863 - val_accuracy: 0.0142\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4870 - accuracy: 0.0161 - val_loss: 1.5867 - val_accuracy: 0.0111\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4952 - accuracy: 0.0156 - val_loss: 1.5902 - val_accuracy: 0.0147\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4874 - accuracy: 0.0161 - val_loss: 1.5875 - val_accuracy: 0.0142\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4869 - accuracy: 0.0163 - val_loss: 1.5859 - val_accuracy: 0.0140\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4925 - accuracy: 0.0160 - val_loss: 1.5861 - val_accuracy: 0.0146\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4853 - accuracy: 0.0162 - val_loss: 1.5907 - val_accuracy: 0.0149\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4855 - accuracy: 0.0161 - val_loss: 1.5866 - val_accuracy: 0.0146\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4849 - accuracy: 0.0163 - val_loss: 1.5854 - val_accuracy: 0.0146\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4818 - accuracy: 0.0163 - val_loss: 1.5891 - val_accuracy: 0.0144\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4812 - accuracy: 0.0162 - val_loss: 1.5887 - val_accuracy: 0.0145\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4905 - accuracy: 0.0163 - val_loss: 1.5836 - val_accuracy: 0.0148\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4887 - accuracy: 0.0165 - val_loss: 1.5871 - val_accuracy: 0.0146\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4843 - accuracy: 0.0163 - val_loss: 1.5877 - val_accuracy: 0.0143\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4827 - accuracy: 0.0163 - val_loss: 1.5857 - val_accuracy: 0.0147\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4820 - accuracy: 0.0163 - val_loss: 1.5852 - val_accuracy: 0.0147\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4808 - accuracy: 0.0164 - val_loss: 1.5892 - val_accuracy: 0.0146\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4797 - accuracy: 0.0162 - val_loss: 1.5874 - val_accuracy: 0.0146\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4791 - accuracy: 0.0163 - val_loss: 1.5887 - val_accuracy: 0.0146\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4784 - accuracy: 0.0163 - val_loss: 1.5923 - val_accuracy: 0.0154\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4774 - accuracy: 0.0162 - val_loss: 1.5935 - val_accuracy: 0.0152\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4760 - accuracy: 0.0164 - val_loss: 1.5905 - val_accuracy: 0.0148\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4757 - accuracy: 0.0165 - val_loss: 1.5865 - val_accuracy: 0.0144\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4718 - accuracy: 0.0165 - val_loss: 1.5879 - val_accuracy: 0.0142\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4703 - accuracy: 0.0167 - val_loss: 1.5914 - val_accuracy: 0.0143\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4705 - accuracy: 0.0166 - val_loss: 1.5866 - val_accuracy: 0.0150\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4688 - accuracy: 0.0168 - val_loss: 1.5861 - val_accuracy: 0.0152\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4683 - accuracy: 0.0169 - val_loss: 1.5875 - val_accuracy: 0.0120\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4672 - accuracy: 0.0168 - val_loss: 1.5903 - val_accuracy: 0.0170\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4657 - accuracy: 0.0168 - val_loss: 1.5877 - val_accuracy: 0.0151\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4677 - accuracy: 0.0169 - val_loss: 1.5853 - val_accuracy: 0.0149\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4695 - accuracy: 0.0167 - val_loss: 1.5845 - val_accuracy: 0.0152\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4648 - accuracy: 0.0169 - val_loss: 1.5880 - val_accuracy: 0.0153\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4613 - accuracy: 0.0169 - val_loss: 1.5913 - val_accuracy: 0.0156\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4599 - accuracy: 0.0170 - val_loss: 1.5832 - val_accuracy: 0.0149\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4598 - accuracy: 0.0172 - val_loss: 1.5920 - val_accuracy: 0.0158\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4578 - accuracy: 0.0173 - val_loss: 1.5939 - val_accuracy: 0.0159\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4561 - accuracy: 0.0174 - val_loss: 1.5872 - val_accuracy: 0.0156\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4558 - accuracy: 0.0172 - val_loss: 1.5849 - val_accuracy: 0.0152\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4517 - accuracy: 0.0173 - val_loss: 1.5921 - val_accuracy: 0.0157\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4575 - accuracy: 0.0173 - val_loss: 1.5844 - val_accuracy: 0.0156\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4514 - accuracy: 0.0175 - val_loss: 1.5886 - val_accuracy: 0.0157\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 1.4482 - accuracy: 0.0182 - val_loss: 1.5853 - val_accuracy: 0.0158\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4492 - accuracy: 0.0180 - val_loss: 1.5902 - val_accuracy: 0.0161\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4478 - accuracy: 0.0182 - val_loss: 1.5849 - val_accuracy: 0.0165\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4483 - accuracy: 0.0183 - val_loss: 1.5900 - val_accuracy: 0.0169\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4441 - accuracy: 0.0182 - val_loss: 1.5844 - val_accuracy: 0.0166\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4406 - accuracy: 0.0184 - val_loss: 1.5856 - val_accuracy: 0.0163\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4412 - accuracy: 0.0185 - val_loss: 1.5862 - val_accuracy: 0.0166\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4372 - accuracy: 0.0185 - val_loss: 1.5911 - val_accuracy: 0.0179\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4395 - accuracy: 0.0187 - val_loss: 1.5893 - val_accuracy: 0.0165\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4433 - accuracy: 0.0182 - val_loss: 1.5879 - val_accuracy: 0.0169\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4354 - accuracy: 0.0191 - val_loss: 1.5912 - val_accuracy: 0.0165\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4367 - accuracy: 0.0194 - val_loss: 1.5908 - val_accuracy: 0.0166\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4332 - accuracy: 0.0189 - val_loss: 1.5841 - val_accuracy: 0.0166\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4343 - accuracy: 0.0188 - val_loss: 1.5789 - val_accuracy: 0.0171\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4321 - accuracy: 0.0193 - val_loss: 1.5827 - val_accuracy: 0.0170\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4279 - accuracy: 0.0197 - val_loss: 1.5813 - val_accuracy: 0.0172\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4334 - accuracy: 0.0195 - val_loss: 1.5832 - val_accuracy: 0.0178\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4337 - accuracy: 0.0195 - val_loss: 1.5825 - val_accuracy: 0.0174\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4336 - accuracy: 0.0199 - val_loss: 1.5817 - val_accuracy: 0.0165\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4422 - accuracy: 0.0189 - val_loss: 1.5762 - val_accuracy: 0.0167\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4243 - accuracy: 0.0203 - val_loss: 1.5761 - val_accuracy: 0.0176\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4157 - accuracy: 0.0208 - val_loss: 1.5795 - val_accuracy: 0.0178\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4310 - accuracy: 0.0197 - val_loss: 1.5813 - val_accuracy: 0.0173\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4174 - accuracy: 0.0211 - val_loss: 1.5765 - val_accuracy: 0.0181\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4236 - accuracy: 0.0208 - val_loss: 1.5846 - val_accuracy: 0.0175\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4093 - accuracy: 0.0217 - val_loss: 1.5819 - val_accuracy: 0.0175\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4093 - accuracy: 0.0211 - val_loss: 1.5860 - val_accuracy: 0.0163\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4098 - accuracy: 0.0218 - val_loss: 1.5787 - val_accuracy: 0.0182\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4296 - accuracy: 0.0203 - val_loss: 1.5811 - val_accuracy: 0.0166\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4239 - accuracy: 0.0212 - val_loss: 1.5778 - val_accuracy: 0.0182\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4020 - accuracy: 0.0227 - val_loss: 1.5846 - val_accuracy: 0.0196\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4070 - accuracy: 0.0214 - val_loss: 1.5805 - val_accuracy: 0.0159\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4101 - accuracy: 0.0222 - val_loss: 1.5784 - val_accuracy: 0.0168\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4005 - accuracy: 0.0228 - val_loss: 1.5810 - val_accuracy: 0.0184\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4088 - accuracy: 0.0222 - val_loss: 1.5847 - val_accuracy: 0.0177\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4062 - accuracy: 0.0225 - val_loss: 1.5834 - val_accuracy: 0.0183\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.4036 - accuracy: 0.0231 - val_loss: 1.5767 - val_accuracy: 0.0181\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3903 - accuracy: 0.0235 - val_loss: 1.5863 - val_accuracy: 0.0199\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3830 - accuracy: 0.0242 - val_loss: 1.5891 - val_accuracy: 0.0194\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.4027 - accuracy: 0.0229 - val_loss: 1.5837 - val_accuracy: 0.0184\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3915 - accuracy: 0.0236 - val_loss: 1.5775 - val_accuracy: 0.0169\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3771 - accuracy: 0.0243 - val_loss: 1.5860 - val_accuracy: 0.0171\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3787 - accuracy: 0.0245 - val_loss: 1.5868 - val_accuracy: 0.0184\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3781 - accuracy: 0.0247 - val_loss: 1.5820 - val_accuracy: 0.0187\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3699 - accuracy: 0.0248 - val_loss: 1.5834 - val_accuracy: 0.0183\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3670 - accuracy: 0.0248 - val_loss: 1.5830 - val_accuracy: 0.0178\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3591 - accuracy: 0.0255 - val_loss: 1.5906 - val_accuracy: 0.0191\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3683 - accuracy: 0.0249 - val_loss: 1.5812 - val_accuracy: 0.0193\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3653 - accuracy: 0.0255 - val_loss: 1.5844 - val_accuracy: 0.0183\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3541 - accuracy: 0.0260 - val_loss: 1.5807 - val_accuracy: 0.0187\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3850 - accuracy: 0.0242 - val_loss: 1.5868 - val_accuracy: 0.0178\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3589 - accuracy: 0.0257 - val_loss: 1.5811 - val_accuracy: 0.0191\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3689 - accuracy: 0.0244 - val_loss: 1.5832 - val_accuracy: 0.0189\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3934 - accuracy: 0.0239 - val_loss: 1.5897 - val_accuracy: 0.0180\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3474 - accuracy: 0.0266 - val_loss: 1.5855 - val_accuracy: 0.0197\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3373 - accuracy: 0.0273 - val_loss: 1.5952 - val_accuracy: 0.0181\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3490 - accuracy: 0.0267 - val_loss: 1.5889 - val_accuracy: 0.0194\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3373 - accuracy: 0.0272 - val_loss: 1.6047 - val_accuracy: 0.0175\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3355 - accuracy: 0.0279 - val_loss: 1.5833 - val_accuracy: 0.0183\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3212 - accuracy: 0.0289 - val_loss: 1.5937 - val_accuracy: 0.0195\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3371 - accuracy: 0.0275 - val_loss: 1.5934 - val_accuracy: 0.0206\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3157 - accuracy: 0.0285 - val_loss: 1.5870 - val_accuracy: 0.0207\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3083 - accuracy: 0.0293 - val_loss: 1.5862 - val_accuracy: 0.0192\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3062 - accuracy: 0.0295 - val_loss: 1.5910 - val_accuracy: 0.0191\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.3015 - accuracy: 0.0299 - val_loss: 1.5969 - val_accuracy: 0.0197\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.3131 - accuracy: 0.0291 - val_loss: 1.5929 - val_accuracy: 0.0189\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2924 - accuracy: 0.0311 - val_loss: 1.5991 - val_accuracy: 0.0199\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2905 - accuracy: 0.0318 - val_loss: 1.6000 - val_accuracy: 0.0205\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2845 - accuracy: 0.0326 - val_loss: 1.5976 - val_accuracy: 0.0197\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2804 - accuracy: 0.0319 - val_loss: 1.5964 - val_accuracy: 0.0198\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2875 - accuracy: 0.0323 - val_loss: 1.5974 - val_accuracy: 0.0195\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2731 - accuracy: 0.0339 - val_loss: 1.5993 - val_accuracy: 0.0202\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2676 - accuracy: 0.0338 - val_loss: 1.6013 - val_accuracy: 0.0205\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2626 - accuracy: 0.0339 - val_loss: 1.5982 - val_accuracy: 0.0190\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2605 - accuracy: 0.0342 - val_loss: 1.5954 - val_accuracy: 0.0189\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2528 - accuracy: 0.0353 - val_loss: 1.6041 - val_accuracy: 0.0209\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2478 - accuracy: 0.0353 - val_loss: 1.6001 - val_accuracy: 0.0190\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2428 - accuracy: 0.0358 - val_loss: 1.6082 - val_accuracy: 0.0192\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2385 - accuracy: 0.0366 - val_loss: 1.6041 - val_accuracy: 0.0188\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2369 - accuracy: 0.0367 - val_loss: 1.6059 - val_accuracy: 0.0194\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2282 - accuracy: 0.0376 - val_loss: 1.6074 - val_accuracy: 0.0194\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2216 - accuracy: 0.0378 - val_loss: 1.6025 - val_accuracy: 0.0197\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2175 - accuracy: 0.0389 - val_loss: 1.6106 - val_accuracy: 0.0201\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2132 - accuracy: 0.0396 - val_loss: 1.6174 - val_accuracy: 0.0215\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.2069 - accuracy: 0.0404 - val_loss: 1.6102 - val_accuracy: 0.0190\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.2026 - accuracy: 0.0409 - val_loss: 1.6065 - val_accuracy: 0.0202\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1983 - accuracy: 0.0413 - val_loss: 1.6125 - val_accuracy: 0.0209\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1897 - accuracy: 0.0422 - val_loss: 1.6107 - val_accuracy: 0.0213\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1882 - accuracy: 0.0424 - val_loss: 1.6088 - val_accuracy: 0.0198\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1798 - accuracy: 0.0432 - val_loss: 1.6022 - val_accuracy: 0.0207\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1752 - accuracy: 0.0437 - val_loss: 1.6264 - val_accuracy: 0.0192\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1690 - accuracy: 0.0442 - val_loss: 1.6196 - val_accuracy: 0.0194\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1643 - accuracy: 0.0455 - val_loss: 1.6184 - val_accuracy: 0.0197\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1599 - accuracy: 0.0464 - val_loss: 1.6224 - val_accuracy: 0.0188\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1503 - accuracy: 0.0470 - val_loss: 1.6154 - val_accuracy: 0.0206\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1480 - accuracy: 0.0479 - val_loss: 1.6231 - val_accuracy: 0.0187\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1444 - accuracy: 0.0487 - val_loss: 1.6209 - val_accuracy: 0.0194\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1406 - accuracy: 0.0481 - val_loss: 1.6230 - val_accuracy: 0.0185\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1338 - accuracy: 0.0500 - val_loss: 1.6321 - val_accuracy: 0.0195\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1297 - accuracy: 0.0510 - val_loss: 1.6258 - val_accuracy: 0.0191\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.1244 - accuracy: 0.0510 - val_loss: 1.6312 - val_accuracy: 0.0190\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1197 - accuracy: 0.0521 - val_loss: 1.6297 - val_accuracy: 0.0207\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1115 - accuracy: 0.0526 - val_loss: 1.6381 - val_accuracy: 0.0189\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1082 - accuracy: 0.0530 - val_loss: 1.6300 - val_accuracy: 0.0194\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.1038 - accuracy: 0.0533 - val_loss: 1.6402 - val_accuracy: 0.0191\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0964 - accuracy: 0.0542 - val_loss: 1.6211 - val_accuracy: 0.0194\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0930 - accuracy: 0.0553 - val_loss: 1.6327 - val_accuracy: 0.0190\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0862 - accuracy: 0.0569 - val_loss: 1.6395 - val_accuracy: 0.0193\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0805 - accuracy: 0.0564 - val_loss: 1.6350 - val_accuracy: 0.0195\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0761 - accuracy: 0.0573 - val_loss: 1.6290 - val_accuracy: 0.0191\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0717 - accuracy: 0.0581 - val_loss: 1.6386 - val_accuracy: 0.0197\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0703 - accuracy: 0.0578 - val_loss: 1.6431 - val_accuracy: 0.0195\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0601 - accuracy: 0.0596 - val_loss: 1.6426 - val_accuracy: 0.0188\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0561 - accuracy: 0.0608 - val_loss: 1.6307 - val_accuracy: 0.0193\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0521 - accuracy: 0.0611 - val_loss: 1.6479 - val_accuracy: 0.0193\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0477 - accuracy: 0.0618 - val_loss: 1.6473 - val_accuracy: 0.0198\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0402 - accuracy: 0.0631 - val_loss: 1.6493 - val_accuracy: 0.0218\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0382 - accuracy: 0.0637 - val_loss: 1.6530 - val_accuracy: 0.0196\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0291 - accuracy: 0.0643 - val_loss: 1.6492 - val_accuracy: 0.0187\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 1.0251 - accuracy: 0.0648 - val_loss: 1.6495 - val_accuracy: 0.0205\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0205 - accuracy: 0.0660 - val_loss: 1.6530 - val_accuracy: 0.0189\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0128 - accuracy: 0.0666 - val_loss: 1.6604 - val_accuracy: 0.0212\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0120 - accuracy: 0.0667 - val_loss: 1.6504 - val_accuracy: 0.0191\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0046 - accuracy: 0.0673 - val_loss: 1.6607 - val_accuracy: 0.0181\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 1.0040 - accuracy: 0.0679 - val_loss: 1.6639 - val_accuracy: 0.0192\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9979 - accuracy: 0.0693 - val_loss: 1.6534 - val_accuracy: 0.0193\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9929 - accuracy: 0.0691 - val_loss: 1.6664 - val_accuracy: 0.0193\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9866 - accuracy: 0.0707 - val_loss: 1.6629 - val_accuracy: 0.0196\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9827 - accuracy: 0.0716 - val_loss: 1.6620 - val_accuracy: 0.0188\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9805 - accuracy: 0.0715 - val_loss: 1.6613 - val_accuracy: 0.0211\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9757 - accuracy: 0.0723 - val_loss: 1.6607 - val_accuracy: 0.0201\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9702 - accuracy: 0.0730 - val_loss: 1.6697 - val_accuracy: 0.0192\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9659 - accuracy: 0.0737 - val_loss: 1.6696 - val_accuracy: 0.0194\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9592 - accuracy: 0.0751 - val_loss: 1.6729 - val_accuracy: 0.0192\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9555 - accuracy: 0.0753 - val_loss: 1.6580 - val_accuracy: 0.0191\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9535 - accuracy: 0.0752 - val_loss: 1.6681 - val_accuracy: 0.0190\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9495 - accuracy: 0.0758 - val_loss: 1.6704 - val_accuracy: 0.0198\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9441 - accuracy: 0.0770 - val_loss: 1.6652 - val_accuracy: 0.0194\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9393 - accuracy: 0.0780 - val_loss: 1.6775 - val_accuracy: 0.0193\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9360 - accuracy: 0.0792 - val_loss: 1.6736 - val_accuracy: 0.0192\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9307 - accuracy: 0.0793 - val_loss: 1.6680 - val_accuracy: 0.0194\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9238 - accuracy: 0.0801 - val_loss: 1.6790 - val_accuracy: 0.0195\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9299 - accuracy: 0.0793 - val_loss: 1.6742 - val_accuracy: 0.0199\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9188 - accuracy: 0.0806 - val_loss: 1.6821 - val_accuracy: 0.0206\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.9167 - accuracy: 0.0819 - val_loss: 1.6851 - val_accuracy: 0.0193\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9090 - accuracy: 0.0820 - val_loss: 1.6773 - val_accuracy: 0.0194\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.9056 - accuracy: 0.0832 - val_loss: 1.6878 - val_accuracy: 0.0195\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8992 - accuracy: 0.0843 - val_loss: 1.6945 - val_accuracy: 0.0211\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8984 - accuracy: 0.0844 - val_loss: 1.6831 - val_accuracy: 0.0199\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8930 - accuracy: 0.0854 - val_loss: 1.6938 - val_accuracy: 0.0197\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8875 - accuracy: 0.0866 - val_loss: 1.6861 - val_accuracy: 0.0201\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8858 - accuracy: 0.0869 - val_loss: 1.6880 - val_accuracy: 0.0189\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8843 - accuracy: 0.0874 - val_loss: 1.6934 - val_accuracy: 0.0194\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8811 - accuracy: 0.0876 - val_loss: 1.6900 - val_accuracy: 0.0190\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8724 - accuracy: 0.0890 - val_loss: 1.6994 - val_accuracy: 0.0187\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8706 - accuracy: 0.0893 - val_loss: 1.6978 - val_accuracy: 0.0193\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8683 - accuracy: 0.0894 - val_loss: 1.6983 - val_accuracy: 0.0195\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8622 - accuracy: 0.0912 - val_loss: 1.7087 - val_accuracy: 0.0194\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8577 - accuracy: 0.0916 - val_loss: 1.7024 - val_accuracy: 0.0194\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8527 - accuracy: 0.0922 - val_loss: 1.7063 - val_accuracy: 0.0189\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8505 - accuracy: 0.0925 - val_loss: 1.7113 - val_accuracy: 0.0197\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8471 - accuracy: 0.0931 - val_loss: 1.7020 - val_accuracy: 0.0185\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8415 - accuracy: 0.0948 - val_loss: 1.7132 - val_accuracy: 0.0218\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8385 - accuracy: 0.0949 - val_loss: 1.7186 - val_accuracy: 0.0194\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8358 - accuracy: 0.0954 - val_loss: 1.6986 - val_accuracy: 0.0189\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8335 - accuracy: 0.0957 - val_loss: 1.7091 - val_accuracy: 0.0194\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8285 - accuracy: 0.0964 - val_loss: 1.7158 - val_accuracy: 0.0196\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8286 - accuracy: 0.0973 - val_loss: 1.7124 - val_accuracy: 0.0193\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8235 - accuracy: 0.0983 - val_loss: 1.7001 - val_accuracy: 0.0192\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8189 - accuracy: 0.0995 - val_loss: 1.7247 - val_accuracy: 0.0201\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8184 - accuracy: 0.0993 - val_loss: 1.7191 - val_accuracy: 0.0192\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.8126 - accuracy: 0.0996 - val_loss: 1.7283 - val_accuracy: 0.0196\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8089 - accuracy: 0.0998 - val_loss: 1.7175 - val_accuracy: 0.0192\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8054 - accuracy: 0.1006 - val_loss: 1.7295 - val_accuracy: 0.0190\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8007 - accuracy: 0.1014 - val_loss: 1.7154 - val_accuracy: 0.0201\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8051 - accuracy: 0.1009 - val_loss: 1.7260 - val_accuracy: 0.0210\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7924 - accuracy: 0.1031 - val_loss: 1.7189 - val_accuracy: 0.0198\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7962 - accuracy: 0.1023 - val_loss: 1.7221 - val_accuracy: 0.0187\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7853 - accuracy: 0.1044 - val_loss: 1.7328 - val_accuracy: 0.0197\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7851 - accuracy: 0.1041 - val_loss: 1.7302 - val_accuracy: 0.0180\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7858 - accuracy: 0.1042 - val_loss: 1.7461 - val_accuracy: 0.0194\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7811 - accuracy: 0.1052 - val_loss: 1.7392 - val_accuracy: 0.0190\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7771 - accuracy: 0.1062 - val_loss: 1.7424 - val_accuracy: 0.0202\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7716 - accuracy: 0.1069 - val_loss: 1.7346 - val_accuracy: 0.0190\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7678 - accuracy: 0.1076 - val_loss: 1.7246 - val_accuracy: 0.0189\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7700 - accuracy: 0.1073 - val_loss: 1.7458 - val_accuracy: 0.0193\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7650 - accuracy: 0.1088 - val_loss: 1.7503 - val_accuracy: 0.0193\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7581 - accuracy: 0.1104 - val_loss: 1.7372 - val_accuracy: 0.0188\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7641 - accuracy: 0.1091 - val_loss: 1.7372 - val_accuracy: 0.0188\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7575 - accuracy: 0.1097 - val_loss: 1.7406 - val_accuracy: 0.0189\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7534 - accuracy: 0.1109 - val_loss: 1.7532 - val_accuracy: 0.0188\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7486 - accuracy: 0.1120 - val_loss: 1.7463 - val_accuracy: 0.0207\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7469 - accuracy: 0.1127 - val_loss: 1.7516 - val_accuracy: 0.0198\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7450 - accuracy: 0.1121 - val_loss: 1.7375 - val_accuracy: 0.0189\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7407 - accuracy: 0.1129 - val_loss: 1.7523 - val_accuracy: 0.0206\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7389 - accuracy: 0.1138 - val_loss: 1.7576 - val_accuracy: 0.0209\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7353 - accuracy: 0.1144 - val_loss: 1.7535 - val_accuracy: 0.0186\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7340 - accuracy: 0.1152 - val_loss: 1.7414 - val_accuracy: 0.0201\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7321 - accuracy: 0.1148 - val_loss: 1.7604 - val_accuracy: 0.0188\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7288 - accuracy: 0.1150 - val_loss: 1.7493 - val_accuracy: 0.0187\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7262 - accuracy: 0.1151 - val_loss: 1.7557 - val_accuracy: 0.0190\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7277 - accuracy: 0.1151 - val_loss: 1.7565 - val_accuracy: 0.0187\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7175 - accuracy: 0.1169 - val_loss: 1.7591 - val_accuracy: 0.0187\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 4s 46ms/step - loss: 0.7147 - accuracy: 0.1181 - val_loss: 1.7585 - val_accuracy: 0.0211\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7134 - accuracy: 0.1188 - val_loss: 1.7632 - val_accuracy: 0.0188\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7118 - accuracy: 0.1197 - val_loss: 1.7544 - val_accuracy: 0.0183\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7074 - accuracy: 0.1194 - val_loss: 1.7665 - val_accuracy: 0.0208\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7038 - accuracy: 0.1193 - val_loss: 1.7551 - val_accuracy: 0.0184\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.7024 - accuracy: 0.1200 - val_loss: 1.7770 - val_accuracy: 0.0194\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.7014 - accuracy: 0.1198 - val_loss: 1.7703 - val_accuracy: 0.0188\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6960 - accuracy: 0.1208 - val_loss: 1.7730 - val_accuracy: 0.0202\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.6981 - accuracy: 0.1213 - val_loss: 1.7739 - val_accuracy: 0.0185\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 4s 46ms/step - loss: 0.6943 - accuracy: 0.1210 - val_loss: 1.7484 - val_accuracy: 0.0187\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6959 - accuracy: 0.1212 - val_loss: 1.7734 - val_accuracy: 0.0194\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.6884 - accuracy: 0.1226 - val_loss: 1.7828 - val_accuracy: 0.0186\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6862 - accuracy: 0.1239 - val_loss: 1.7827 - val_accuracy: 0.0199\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6854 - accuracy: 0.1244 - val_loss: 1.7739 - val_accuracy: 0.0199\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6824 - accuracy: 0.1246 - val_loss: 1.7885 - val_accuracy: 0.0185\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6804 - accuracy: 0.1243 - val_loss: 1.7807 - val_accuracy: 0.0190\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6760 - accuracy: 0.1261 - val_loss: 1.7660 - val_accuracy: 0.0192\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6738 - accuracy: 0.1255 - val_loss: 1.7784 - val_accuracy: 0.0185\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6704 - accuracy: 0.1258 - val_loss: 1.7770 - val_accuracy: 0.0183\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6699 - accuracy: 0.1258 - val_loss: 1.7814 - val_accuracy: 0.0185\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.6645 - accuracy: 0.1269 - val_loss: 1.7960 - val_accuracy: 0.0185\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6643 - accuracy: 0.1266 - val_loss: 1.7871 - val_accuracy: 0.0207\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6604 - accuracy: 0.1274 - val_loss: 1.7942 - val_accuracy: 0.0186\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6651 - accuracy: 0.1268 - val_loss: 1.7998 - val_accuracy: 0.0193\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6528 - accuracy: 0.1296 - val_loss: 1.7863 - val_accuracy: 0.0190\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.6514 - accuracy: 0.1300 - val_loss: 1.7863 - val_accuracy: 0.0192\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6517 - accuracy: 0.1293 - val_loss: 1.8015 - val_accuracy: 0.0190\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.6527 - accuracy: 0.1297 - val_loss: 1.7809 - val_accuracy: 0.0188\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 0.6501 - accuracy: 0.1313 - val_loss: 1.8010 - val_accuracy: 0.0181\n",
            "Epoch 295/500\n",
            " 9/80 [==>...........................] - ETA: 2s - loss: 0.6128 - accuracy: 0.1263"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-15b736664725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'temporal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZxR-20DvbVK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "loss=history1.history['loss']\n",
        "val_loss=history1.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVjdY1_syVNo"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-BO7-WLPW9i"
      },
      "source": [
        "from keras.models import load_model\n",
        "training_model = load_model('training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYOVJPCPW5-"
      },
      "source": [
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7rEX_--PW3a"
      },
      "source": [
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOYDxa10OeZ6"
      },
      "source": [
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGLxFzOXOeW3"
      },
      "source": [
        "from keras.models import load_model\n",
        "training_model = load_model('training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_response(test_input):\n",
        "    #Getting the output states to pass into the decoder\n",
        "    states_value = encoder_model.predict(test_input)\n",
        "    \n",
        "    #Generating empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    \n",
        "    #Setting the first token of target sequence with the start token\n",
        "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "    \n",
        "    #A variable to store our response word by word\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "          #Predicting output tokens with probabilities and states\n",
        "          output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
        "          \n",
        "          #Choosing the one with highest probability\n",
        "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "          sampled_token = reverse_target_features_dict[sampled_token_index]\n",
        "          decoded_sentence += \" \" + sampled_token\n",
        "          \n",
        "          #Stop if hit max length or found the stop token\n",
        "          if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "          \n",
        "          #Update the target sequence\n",
        "          target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "          target_seq[0, 0, sampled_token_index] = 1.\n",
        "          \n",
        "          #Update states\n",
        "          states_value = [hidden_state, cell_state]\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_haf70rOeUf"
      },
      "source": [
        "class ChatBot:\n",
        "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
        "  \n",
        "  #Method to start the conversation\n",
        "  def start_chat(self):\n",
        "    user_response = input(\"Hi, I'm a chatbot trained on random dialogs. AMA!\\n\")\n",
        "    \n",
        "    if user_response in self.negative_responses:\n",
        "      print(\"Ok, have a great day!\")\n",
        "      return\n",
        "    self.chat(user_response)\n",
        "  \n",
        "  #Method to handle the conversation\n",
        "  def chat(self, reply):\n",
        "    while not self.make_exit(reply):\n",
        "      reply = input(self.generate_response(reply)+\"\\n\")\n",
        "    \n",
        "  #Method to convert user input into a matrix\n",
        "  def string_to_matrix(self, user_input):\n",
        "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
        "    user_input_matrix = np.zeros(\n",
        "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "      dtype='float32')\n",
        "    for timestep, token in enumerate(tokens):\n",
        "      if token in input_features_dict:\n",
        "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
        "    return user_input_matrix\n",
        "  \n",
        "  #Method that will create a response using seq2seq model we built\n",
        "  def generate_response(self, user_input):\n",
        "    input_matrix = self.string_to_matrix(user_input)\n",
        "    chatbot_response = decode_response(input_matrix)\n",
        "    #Remove <START> and <END> tokens from chatbot_response\n",
        "    chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
        "    return chatbot_response\n",
        "  \n",
        "  #Method to check for exit commands\n",
        "  def make_exit(self, reply):\n",
        "    for exit_command in self.exit_commands:\n",
        "      if exit_command in reply:\n",
        "        print(\"Ok, have a great day!\")\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "chatbot = ChatBot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex0L82Zh4nC1"
      },
      "source": [
        "## Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORcESPLnr2gS"
      },
      "source": [
        "chatbot.start_chat()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}